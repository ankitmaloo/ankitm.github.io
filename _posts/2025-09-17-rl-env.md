---
layout: post
title: "Notes on RL Environments"
date: 2025-09-19
---
*AI trends move fast. Writing about them makes sense if it’s your job. I’m picking this up because it’s [escalated](https://x.com/willccbb/status/1967728417526386712) recently, and some discussion seems needed. This is me trying to make sense of an RL environment (and this understanding may evolve).*

This is pretty much sparked by a few things. Some [RL](https://techcrunch.com/2025/09/16/silicon-valley-bets-big-on-environments-to-train-ai-agents/) [environment](https://www.nytimes.com/2025/06/11/technology/ai-mechanize-jobs.html) startups are getting hot, others are using [environments](https://x.com/vincentweisser/status/1961594111733158141) as an open source growth engine. Some [cautioned](https://benanderson.work/blog/dont-build-rl-env-startup/) against building env startup as well. I will leave you to read these linked posts to understand the basics. 

> tl;dr: Environments are valuable until priors saturate. Durable moats come from fresh, proprietary feedback loops or predictive reward models that reflect shifting reality.

## What is an RL environment?

An environment supplies observations, accepts actions, emits rewards, and transitions state for a given RL setup. Algorithms (PPO, RLHF/DPO variants) optimize behavior within that environment. Evals are environments without learning turned on.

## Why verifiable-reward RL works now?

RL can be understood as a function of three levers: environment (where the reward comes from), algorithm (how you propagate the reward), and experience/training set of the base model (referred to as 'prior' in both Bayesian and RL literature). All throughout the RL research, we have focused heavily on algorithms, coming up with new ways to propagate the reward and always starting from scratch. In this [excellent post](https://ysymyth.github.io/The-Second-Half/) called *The Second Half* by Shunyu, he breaks down how priors are the most important aspect of the three, but we had no way of getting there previously. 

Over the last five years, we scaled pretraining to the entire internet and more. Models knew about all topics, and could respond intelligently with details. But, something was missing - an ability to make sense of all that pretrained knowledge. This is where Chain of Thought (CoT) came in. It lets models (not unlike humans) connect the dots and generalize from what they know. When you give a model compute to think before acting, it can use its knowledge in important ways. As Shunyu says: 

> language generalizes through reasoning in agents

So the key to making models better at tasks comes down to this: **providing the right priors for the given task[^1]**. With enough context and ability to reason, the model will generalize and will figure out how to solve those tasks.  

**Environments and algorithms are tools to elicit or update these priors**[^2]. A language model with strong priors and high ttc to think will be able to finish related tasks. Moreover, we can teach a model new priors by designing the right set of tasks or evals. 

## Environments can generate training examples

Environments are particularly good at generating training examples quickly (which then become priors). But the catch is, once the model has enough training to generalize, the marginal utility of the environment goes from being critical to an eval harness. They remain valuable for evaluations, safety/regression tests, and incremental improvements. Say web browsing[^3]: 

- Base models have scant pretraining on DOM trajectories → Priors are almost zero. 
- SFT is expensive because annotating “click at (x,y) because CSS selector …” is labour-heavy and site-specific.
- An environment (playwright sandbox + reward = task success) can auto-generate thousands of trajectories per GPU-day. 
- With ball-park priors the model can generalise to new sites after ≈ 10k env steps (for example).
- Hence, today, you need the environment to create the prior[^4]. 

Or, you can also go through other routes: 
- Synthetic text distillation without an environment => A teacher model generates DOM-action pairs filled with static checks, and added to SFT. ([Adept](https://www.adept.ai/blog/adept-agents) did it and called it verbal web simulator)
- Cross modal transfer: Recording videos of people clicking websites (youtube + some data companies in India). Creates a visual prior without needing an env. 

- One other way to obtain priors is simply use a model which has already been trained on web browsing or for using particular websites. First movers are severely disadvantaged in this space.

Once a model has these priors, it just needs reasoning ability and interaction with live websites. You don't need special environments for every new website. They are now only useful for evaluations or safety training.

## When environments create value vs collapse to eval harness 

Web browsing example is deliberately chosen because it's simple, crisp, short horizon problem once the action prior exists. Multi turn agentic work is more complex, but the same principle holds though applied differently. You would need a lot more steps, and need to figure out a long horizon credit assignment and final reward. Recipe remains the same: 

- When we start out, the environment is super valuable because the model knows little on how to do the task. 
- We use the environment to quickly generate lots of training examples
- Once priors saturate, the environment becomes an eval harness

So, does this mean all environments eventually become worthless? It depends on whether what the AI needs to learn stays constant or keeps changing. This brings us to an interesting type of environment:

## Where moats come from
### Predictive Reward Environments

The technical term for this is Surrogate Reward Models (SRM)

> When rewards are delayed or subjective, you need a surrogate reward model(SRM) trained on real outcomes. That model is only as good as the breadth, freshness and exclusivity of the data you feed it, maintaining it is the key.

In cases like creating an effective sales pitch, knowing if a strategy document is good enough, or getting a compliance report approved, rewards are subjective and depend on multiple factors. You need environments that can model either human behavior or complex system interactions. 

Here's where predictive reward models come in. In drug discovery, we have models that can look at a protein structure and predict its binding probability, and assign that as a reward, instead of going and testing the structure in a wet lab. It's a cheap method, that is instant, scalable and can model delayed outcomes. 

In business contexts we need models that can predict the "probability of this generated compliance report getting approved?", "likelihood of a business committee preferring one strategy report vs another" and so on. 

However, these are prone to Goodhart's law and reward hacking. Your predictive model is only as good as your access to real world data. The environment becomes an approximation of real world outcomes. The prediction reward model is the product with a durable half life. Your SRM + live data flywheel here is your moat.

## Cursor, Mercor, and Real World "Environments"

While I was writing this, Cursor announced their release of online RL with training via live user interactions and rolling out a new policy every two hours. Then mercor had an x article about a similar thing. For the context of this post, I think the failure modes are different, moats are more about user scale, sampling, and data exclusivity, and it's about consistently changing priors. The product becomes the environment, constantly updating priors with real interactions. 

Though changing priors scenario means that Mercor's claim of "Teaching an AI once is a fixed cost that eliminates the corresponding variable human cost forever.” is likely not going to be widely applicable. You have to keep teaching in many scenarios.

Beyond SRMs, there's another category where environments struggle: preference driven tasks with hidden states.

## What about environments for buying on Amazon or booking a flight/hotel?

I built a very early AI travel chatbot in 2018. Biggest learning from that was different users have different implicit preferences and merely finding the optimal flight/hotel is not worth much. Users want AI chatbots to read their mind. Incorporating their preferences nearly doubled our conversions[^5].

Tasks like booking a hotel or buying on Amazon are part verifiable, and part preference driven. I'll posit that given the value from Amazon's recommendation engine is high, the environments or even the buying workflow would not find many real world takers (unless amazon offers an environment). Decison making as a prior is hard to simulate without the data from the providers.  With flights and hotels, there is added complexity of dynamic pricing and modeling hidden states (inventory etc.). Building these models from scratch is a hard task, but there is huge alpha in that for anyone who can. 

## Conclusion

If you are thinking of building a RL environment startup because everyone is doing it, or models need RL, know the game you are playing. Useful to think in terms of priors and how they change. 

- If you have access to a constant data feed that captures human behavior no one else sees. Build it. Thats a clear moat. 

- If you can map out human preferences in a way they model their buying, approval, or adoption patterns, probably worth a $100B company. 

- Or sometimes, if you are lucky, your environment could become the product itself. Eg: claude code. If you can get the foundational model companies to not enforce their exclusivity contract that is. 

Build for shifting priors with fresh, verifiable feedback. Everything else decays into an eval harness with a shinier marketing term.

---

[^1]: With thinking / test time compute, these priors generalize in a given environment. I see the end goal as to get to a model that can do tasks, with or without RL, so priors are the key, thinking or generalization is an action. Priors can be about knowledge, skill, preference, or even context (retrieval). That is a separate post though. 

[^2]: With backpropagation and verifiable rewards in the right environment, model learns about the specifics on how to solve a given task, and update the priors.

[^3]: From the same Shunyu blog post: 
> Language pre-training created good priors for chatting, but not equally good for controlling computers or playing video games. Why? These domains are further from the distribution of Internet text, and naively doing SFT / RL on these domains generalizes poorly. So you need to add more in training data. 

[^4]: For a task to be done well, the model needs to learn and narrow down which trajectory of generalization helps.

[^5]: I won't go into much details here on how. Long story short, we sent a 25 question [survey](https://forms.gle/HoEsFw1UnduoSLgi7) to every new user, got 500 responses, interviewed them further, and built it.